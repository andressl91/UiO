\documentclass[a4paper,norsk]{article}
\usepackage{preamble}
\usepackage{amsmath}
\begin{document}
\maketitle

We are presented the general problem \\
\textit{Find a k-dimensional subspace W of R m so that the sum of the (squared) distances from a set of n given points x1,x2,. . . ,xn in R m to W is as small as possible}
We define the \textit{matrix of observations} \textbf{X} as \\

\begin{align*}
	\text{\textbf{X}} = 
 \begin{pmatrix}
  x_{1}^{(1)} & x_{2}^{(1)} & \cdots & x_{n}^{(1)} \\
  x_{1}^{(2)} & x_{2}^{(2)} & \cdots & x_{n}^{(2)} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  x_{1}^{(m)} & x_{2}^{(m)} & \cdots & x_{n}^{(m)}
 \end{pmatrix}
\end{align*}

\section*{Problem 1}
First we assume that W is known such that $\{w_i\}_{i=1}^{k}$ is an orthonormal basis for W, and let $\{w_i\}_{i=k+1}^{m}$
be an orthonormal basis for $W^{\perp}$. We define the projection operator 
\begin{align*}
proj_u v = \frac{\langle v , u\rangle}{\langle u, u \rangle} v
\end{align*}
 as the projection of a vector v onto the vector u. \\
Now using this operator to find the projection of the columns of the matrix of observations $x_i$ on the space $W^\perp$ we get 

\begin{align*}
\sum_{i=1}^n ||proj_{W^\perp} x_i||^2 = \sum_{i=1}^n \frac{\langle x_i^T , W^\perp \rangle}{\langle W^\perp, W^\perp \rangle} = 
\sum_{i=1}^n \sum_{i=k+1}^m \frac{\langle x_i^T , w_j \rangle}{\langle w_j, w_j \rangle}
\end{align*}
Since $\{w_i\}_{i=k+1}^{m}$ spans an orthonormal basis for $W^\perp$, by definition $\langle w_i, w_i \rangle = 1$.
From this we can  rewrite the last sum in a more compact form 
as the frobenius norm of the matrix product ||\textbf{$X^T$ W}||, where \textbf{W} is 
the matrix with columns  $\{w_i\}_{i=k+1}^{m}$
\newpage

\section*{Problem 2}
As introduced, if $\textbf{X} = \textbf{U} \Sigma \textbf{V}^*$ is and SVD of \textbf{X} then  $\textbf{X}^T = \textbf{V} \Sigma^T \textbf{U}^*$ we have the relation \newline
\begin{align*}
||\textbf{X}^T \textbf{W}||_F^2 = ||\Sigma^T \textbf{U}^* \textbf{W}||_F^2 = \sum_{i=1}^m \sum_{j=k+1}^m \sigma_i^2  \langle \textbf{u}_i , \textbf{w}_j\rangle^2 =
\sum_{i=1}^m \sigma_i^2 || proj_{w^{\perp}} \textbf{u}_i||^2 
\end{align*}

From SVD we know that the unitary matrix \textbf{U} spans an orthonormal basis of \textbf{X} such that $(\textbf{u}_1, \textbf{u}_2 .... \textbf{u}_k)$ for $k=rank(X)$. Now to minimize 
$||\textbf{X}^T \textbf{W}||_F^2$ we want in some sence to construct W such that W is in the same span as \textbf{X}, hence we want to construct W such that W spans the same space as the
orthonormal basis \textbf{U} of \textbf{X}. \newline \newline
Since W is a k-dimensional subspace of $\mathbb{R}$, we can orientate W such that $\{w_i\}_{i=1}^{k}$ spans the same space as the components $\textbf{u}_i$ for $i=1,2..k$, in other words
$W = span(\textbf{u}_1, ..., \textbf{u}_k )$. \newline
Then $\sum_{i=1}^k || proj_{w^{\perp}} \textbf{u}_i||^2 = 0$ (Mark the upper limit of the sum) \newline
Now $\textbf{u}_i$ for $i=k+1, k+2...m$ will lie in $span(W^\perp)$, and therefore $\sum_{i=k+1}^m || proj_{w^{\perp}} \textbf{u}_i||^2 = m-k$ since both  $\{w_i\}_{i=k+1}^{m}$ and
 $\{u_i\}_{i=k+1}^{m}$ spans an orthonormal basis. \newline \newline
From this argumentation, finding the optimal subspace of W is equivalent to minimizing the system $\sum_{i=1}^m \sigma_i^2 x_i$ which will happen when the conditions
$x_1 = \cdots = x_k = 0$, \hspace{2mm} $x_{k+1} = \cdots = x_m = 1 $ and $\sum_{i=1}^m x_i = m - k$. \newline \newline
And finally since $|| proj_{w^{\perp}} \textbf{u}_i||^2 = 1$ for $i=k+1, k+2...m$, \newline \newline
$\sum_{i=k+1}^m \sigma_i^2 || proj_{w^{\perp}} \textbf{u}_i||^2 = \sum_{i=k+1}^m \sigma_i^2 = ||\textbf{X}^T \textbf{W}||_F^2$


\newpage

\section*{Problem 3}
In exercise 2 we introduced the a vector \textbf{c} to help find the best approximation for the matrix \textbf{X} on \textbf{W} such that the distance from $\textbf{x}_i - \textbf{c}$ to \textbf{W} is as small
as possible. Using the results from exercise 2 we replace $\textbf{x}_i$ with $\textbf{x}_i - \textbf{c}$. This relation is the same as replacing \textbf{X} with 
$\tilde{\textbf{X}} = \textbf{X} - \textbf{c}(1 \hspace{2mm} 1 \cdots \hspace{2mm} 1)$ 
Using this relation we get 
\begin{align*}
&||\tilde{\textbf{X}}^T \textbf{w}_i ||^2 = (\tilde{\textbf{X}}^T \textbf{w}_i)^T (\tilde{\textbf{X}}^T \textbf{w}_i) =  \textbf{w}_i^T \tilde{\textbf{X}} \tilde{\textbf{X}}^T \textbf{w}_i \\
&\textbf{w}_i^T (\textbf{X} - \textbf{c} (1 \hspace{2mm} 1 \cdots 1 )  ) (\textbf{X} - \textbf{c} (1 \hspace{2mm} 1 \cdots 1))^T \textbf{w}_i =\\
&s\textbf{w}_i^T (\textbf{X} - \textbf{c} (1 \hspace{2mm} 1 \cdots 1 )  ) (\textbf{X}^T - (1 \hspace{2mm} 1 \cdots 1)^T \textbf{c}^T) \textbf{w}_i =\\
&\textbf{w}_i^T (\textbf{X}\textbf{X}^T - \textbf{X}(1 \hspace{2mm} 1 \cdots 1)^T\textbf{c}^T - \textbf{c}(1 \hspace{2mm} 1 \cdots 1)\textbf{X}^T +
 \textbf{c}(1 \hspace{2mm} 1 \cdots 1)(1 \hspace{2mm} 1 \cdots 1)^T \textbf{c}^T )\textbf{w}_i 
\end{align*}

\begin{enumerate}
\item $\textbf{w}^T \textbf{X}\textbf{X}^T\textbf{w}$ falls out directly from multiplication
\item From $\textbf{w}_i^T\textbf{c}(1 \hspace{2mm} 1 \cdots 1)(1 \hspace{2mm} 1 \cdots 1)^T \textbf{c}^T \textbf{w}_i$ \newline 
we observe that the matrix multiplication $\textbf{c}(1 \hspace{2mm} 1 \cdots 1) \textbf{c}(1 \hspace{2mm} 1 \cdots 1)^T$ \newline(which is a scalar $(1 \times n) (n \times 1)$)
will just be sum n, and by rearranging the terms we get $n\textbf{c}_i^T \textbf{w}_i \textbf{w}_i^T \textbf{c}$
\item The two final terms can be rewritten into one term. By a closer look at the $\textbf{w}^T \textbf{c}(1 \hspace{2mm} 1 \cdots 1)\textbf{X}^T \textbf{w}$ we change the order
of multiplication and still get the equivalent expression \newline 
$(\sum_{i=1}^n x_i^{(i)} \hspace{2mm} \sum_{i=1}^n x_i^{(i)} \cdots \hspace{2mm} \sum_{i=1}^n x_m^{(i)})\textbf{w}_i\textbf{w}_i^T\textbf{c}$. Equivalent argument yields for the last term
\end{enumerate}

And we find that
\begin{align*}
||\tilde{\textbf{X}}^T \textbf{w}_i ||^2 = n\textbf{c}_i^T \textbf{w}_i \textbf{w}_i^T \textbf{c} - 
2(\sum_{i=1}^n x_1^{(i)} \hspace{2mm} \sum_{i=1}^n x_2^{(i)} \cdots \hspace{2mm} \sum_{i=1}^n x_m^{(i)})\textbf{w}_i\textbf{w}_i^T\textbf{c} +
n\textbf{c}_i^T \textbf{w}_i \textbf{w}_i^T \textbf{c}
\end{align*}

\newpage

\section*{Problem 4}
In general the Lagrange method or method of Lagrange multipliers we want to find a local maxima or minima of a function f, with some constraints evalued from a function g. 
We define a Lagrange function $\mathcal{L}$ such that 
\begin{align*}
\mathcal{L}(x_1, x_2,...,x_n, \lambda_1, ..., \lambda_n) = f(_1, x_2,...,x_n) - \sum_{i=1}^n \lambda_i g_i(x_1, x_2,...,x_n)  
\end{align*}
where $\lambda_i$ yields some scalar value to each constraint. Here the funtion g is limited to the set of points such that
$g(x_1, x_2,...x_n) = 0$
Further from a mathematical evaluation these local maxima or minima occur when the function f and g are parallel, which is equivalent to that the gradient of f and g are paralell. Hence the problem is reduced on the form
\begin{align}
\nabla f(\textbf{x}) = \sum_{i=1}^n \lambda_i \nabla g_i(\textbf{x})
\end{align}

In our case these multiple constraints is defined as $\textbf{w}_i^T \textbf{c} = 0$ for $i = 1, ..,k$ as set in exercise 3 by the assumtion that  vector $\textbf{c} \in W$. Further the function f, generalized to our problem is to minimize 
$||\tilde{\textbf{X}}^T \textbf{W} ||_F^2  $, and the gradient is taken with respect to \textbf{c}, as this is the value of interest to find the best approximation. \newline \newline

Now by (1) we easily see that the left hand side is just the gradient of $||\tilde{\textbf{X}}^T \textbf{W} ||_F^2  $
with respect to \textbf{c} as defined in exercise 3. For the right hand side we get the result by
\begin{align*}
\frac{\partial}{\partial \textbf{c}} g(\textbf{x}) = \frac{\partial}{\partial \textbf{c}} \textbf{w}_i^T c = \textbf{w}_i^T \hspace{4mm} \text{for i = 1, ..., k} \\
\sum_{i=1}^k \lambda_i \nabla g_i(\textbf{x}) = \sum_{i=1}^n \lambda_i \textbf{w}_i^T =
(\textbf{w}_1 \hspace{2mm} \textbf{w}_2 \cdots \textbf{w}_k) 
\begin{pmatrix}\lambda_{1} \\ \lambda_{2} \\ \vdots \\ \lambda_{k} \end{pmatrix}
\end{align*}
\newline \newline
From this result we can show that $\textbf{c} - \bar{\textbf{x}} \in W$ where 
$\bar{\textbf{x}} = \frac{1}{n} \sum_{i=1}^n \textbf{x}_i$. Considering the left hand side we can rewrite the terms using the relation from the Lagrange multiplier
\begin{align*}
&\sum_{i=k+1}^m \Bigg( 2n\textbf{w}_i \textbf{w}_i^T \textbf{c} - 2 \textbf{w} \textbf{w}_i^T 
 \begin{pmatrix} \sum_{i=1}^n x_1^{(i)} \\ \sum_{i=1}^n x_2^{(i)} \\ \vdots \\ \sum_{i=1}^n x_m^{(i)} \end{pmatrix} \Bigg)	= 
\sum_{i=k+1}^m  2\textbf{w}_i \textbf{w}_i^T \Bigg(\textbf{c} -  
 \frac{1}{n} \begin{pmatrix} \sum_{i=1}^n x_1^{(i)} \\ \sum_{i=1}^n x_2^{(i)} \\ \vdots \\ \sum_{i=1}^n x_m^{(i)} \end{pmatrix} \Bigg)	\\
&\sum_{i=k+1}^m  2\textbf{w}_i \textbf{w}_i^T (\textbf{c} - \bar{\textbf{x}})  = \begin{pmatrix} 0 \\ 0 \\ \vdots \\0 \end{pmatrix}
\end{align*}
And since $\textbf{w}_i \in W$ for $i = k+1,...,m$, cleary $\textbf{c} - \bar{\textbf{x}} \in W$ to fulfill the equation.



\end{document}

